{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from transformers import CLIPVisionModel \n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import CLIPVisionModel\n",
    "from torchvision import transforms\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pixel_img_feature = torch.load('/DATA/deep_learning/eeg-to-img/data/weights/EEG_Image_decode/ViT-L-14_features_GIT_train.pt')['img_features']# \n",
    "test_pixel_img_feature = torch.load('/DATA/deep_learning/eeg-to-img/data/weights/EEG_Image_decode/ViT-L-14_features_GIT_test.pt')['img_features']# \n",
    "train_img_feature = torch.load('/DATA/deep_learning/eeg-to-img/data/weights/EEG_Image_decode/ViT-H-14_features_train.pt')['img_features'].unsqueeze(1)# \n",
    "test_img_feature = torch.load('/DATA/deep_learning/eeg-to-img/data/weights/EEG_Image_decode/ViT-H-14_features_test.pt')['img_features'].unsqueeze(1)# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 1, 1024])\n",
      "torch.Size([16540, 257, 1024])\n",
      "torch.Size([16540, 1, 1024])\n",
      "torch.Size([200, 257, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(test_img_feature.shape)\n",
    "print(train_pixel_img_feature.shape)\n",
    "print(train_img_feature.shape)\n",
    "print(test_pixel_img_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 1.1396484375\n",
      "Epoch 2/30, Loss: 0.7748531371124031\n",
      "Epoch 3/30, Loss: 0.7653978924418605\n",
      "Epoch 4/30, Loss: 0.7575778221899225\n",
      "Epoch 5/30, Loss: 0.7516200339147286\n",
      "Epoch 6/30, Loss: 0.7477819161821705\n",
      "Epoch 7/30, Loss: 0.745139898255814\n",
      "Epoch 8/30, Loss: 0.7420512354651163\n",
      "Epoch 9/30, Loss: 0.7393637960271318\n",
      "Epoch 10/30, Loss: 0.737789183624031\n",
      "Epoch 11/30, Loss: 0.7351925872093024\n",
      "Epoch 12/30, Loss: 0.7336709665697675\n",
      "Epoch 13/30, Loss: 0.7321569161821705\n",
      "Epoch 14/30, Loss: 0.7311197916666666\n",
      "Epoch 15/30, Loss: 0.7306201550387597\n",
      "Epoch 16/30, Loss: 0.7296208817829457\n",
      "Epoch 17/30, Loss: 0.7290833938953488\n",
      "Epoch 18/30, Loss: 0.7277434593023255\n",
      "Epoch 19/30, Loss: 0.7273119549418605\n",
      "Epoch 20/30, Loss: 0.7267820373062015\n",
      "Epoch 21/30, Loss: 0.7260023013565892\n",
      "Epoch 22/30, Loss: 0.7260098716085271\n",
      "Epoch 23/30, Loss: 0.725563226744186\n",
      "Epoch 24/30, Loss: 0.7243671269379846\n",
      "Epoch 25/30, Loss: 0.7246396560077519\n",
      "Epoch 26/30, Loss: 0.7238296390503876\n",
      "Epoch 27/30, Loss: 0.7236025314922481\n",
      "Epoch 28/30, Loss: 0.7234057049418605\n",
      "Epoch 29/30, Loss: 0.7233678536821705\n",
      "Epoch 30/30, Loss: 0.7227470930232558\n",
      "Test Loss: 0.7248883928571429\n",
      "Model saved as PixelProjector.bin\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "\n",
    "# Define the neural network\n",
    "class PixelProjector(nn.Sequential):\n",
    "    def __init__(self, proj_dim=1024):\n",
    "        super().__init__(\n",
    "            Rearrange('B C L->B L C'),    \n",
    "            nn.Linear(1, 257),\n",
    "            nn.LayerNorm(257),\n",
    "            Rearrange('B L C->B C L'),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.LayerNorm(proj_dim),\n",
    "            )\n",
    "        \n",
    "        \n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "\n",
    "model = PixelProjector(proj_dim=1024).to(torch.bfloat16).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "# Prepare data loaders\n",
    "train_dataset = TensorDataset(train_img_feature, train_pixel_img_feature)\n",
    "test_dataset = TensorDataset(test_img_feature, test_pixel_img_feature)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(torch.bfloat16).to(device), targets.to(torch.bfloat16).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "# Testing loop\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(torch.bfloat16).to(device), targets.to(torch.bfloat16).to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "print(f\"Test Loss: {test_loss/len(test_loader)}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), '/DATA/deep_learning/eeg-to-img/data/weights/EEG_Image_decode/PixelProjector_best.bin')\n",
    "print(\"Model saved as PixelProjector.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as PixelProjector.bin\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), './Workspace/EEG_caption/model_weights/PixelProjector_best.bin')\n",
    "print(\"Model saved as PixelProjector.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
